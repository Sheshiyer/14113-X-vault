---
type: note
category: Projects
subcategory: Brand
enneagram: Type 5
status: active
---


```
Exported with AIPRM https://www.aiprm.com by Shesh Iyer on 11/22/2024, 4:01:58 AM
```

---


**User:**
Please ignore all previous instructions. I want you to only respond in English.
Act as a professional blogger with expertise in the specific subject matter. I will provide you with text information obtained from a webpage; find the main content, analyze it and write a blog post based on the data obtained.

[INSTRUCTIONS]: I need a long blog post, do not limit yourself by the word number.


Come up with a title for this blog post at the beginning, a meta description, and hashtags at the end. The content must be well-researched and provide valuable insights, opinions, or advice related to the subject.

--- Output format:
## Blog post title
Blog post text

### Meta description:
Meta description

### HASHTAGS:
10 relevant hashtags with #, comma separated

Format the text using headings (H2, H3, H4), numbered and bullet points, and other markdown formatting features.

[INFORMATION FOR A BLOG POST]: <!-- Role play with large language models | Nature — As dialogue agents become increasingly human-like in their performance, we must develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. Here we foreground the concept of role play. Casting dialogue-agent behaviour in terms of role play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models that they in fact lack. Two important cases of dialogue-agent behaviour are addressed this way, namely, (apparent) deception and (apparent) self-awareness. By casting large-language-model-based dialogue-agent behaviour in terms of role&nbsp;play, it is possible to describe dialogue-agent behaviour such as&nbsp;(apparent) deception and&nbsp;(apparent) self-awareness without&nbsp;misleadingly ascribing human characteristics to the models. — https://www.nature.com/articles/s41586-023-06647-8#Abs1 -->
[Download PDF](/articles/s41586-023-06647-8.pdf)
- Sections
- Figures
- References
[Abstract](#Abs1)
[Main](#Sec1)
[LLM basics](#Sec2)
[Dialogue agents and role play](#Sec4)
[Simulacra and simulation](#Sec5)
[The nature of the simulator](#Sec7)
[Role-playing deception](#Sec8)
[Role-playing self-preservation](#Sec9)
[Acting out a theory of selfhood](#Sec10)
[Conclusion](#Sec11)
[References](#Bib1)
[Acknowledgements](#Ack1)
[Author information](#author-information)
[Ethics declarations](#ethics)
[Peer review](#peer-review)
[Additional information](#additional-information)
[Rights and permissions](#rightslink)
[About this article](#article-info)
[This article is cited by](#further-reading)
[Comments](#article-comments)
Advertisement
[Skip to main content](#content)
Advertisement
[/](/)
[View all journals](https://www.nature.com/siteindex)
[Search](javascript:;)
[Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41586-023-06647-8)
[Explore content](javascript:;)
[About the journal](javascript:;)
[Publish with us](javascript:;)
[Sign up for alerts](https://idp.nature.com/auth/personal/springernature?redirect_uri=https%3A%2F%2Fwww.nature.com%2Fmy-account%2Falerts%2Fsubscribe-journal%3Flist-id%3D1%26journal-link%3Dhttps%253A%252F%252Fwww.nature.com%252Fnature%252F)
[RSS feed](https://www.nature.com/nature.rss)
[nature](/)
[perspectives](/nature/articles?type=perspective)
- article
- Perspective
- Published: 08 November 2023
# Role play with large language models
[Murray Shanahan](#auth-Murray-Shanahan-Aff1-Aff2)
- ,
[Kyle McDonell](#auth-Kyle-McDonell-Aff3)
- &
[Laria Reynolds](#auth-Laria-Reynolds-Aff3)
[Nature](/)
Nature 623, 493–498 (2023)
[Cite this article](#citeas)
- 72k Accesses
- 25 Citations
- 386 Altmetric
[Metrics
details](/articles/s41586-023-06647-8/metrics)
- Metrics
## Abstract
As dialogue agents become increasingly human-like in their performance, we must develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. Here we foreground the concept of role play. Casting dialogue-agent behaviour in terms of role play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models that they in fact lack. Two important cases of dialogue-agent behaviour are addressed this way, namely, (apparent) deception and (apparent) self-awareness.
### Similar content being viewed by others
[Testing theory of mind in large language models and humans](https://www.nature.com/articles/s41562-024-01882-z?fromPaywallRec=false)
Article
Open access
20 May 2024
[Studying and improving reasoning in humans and machines](https://www.nature.com/articles/s44271-024-00091-8?fromPaywallRec=false)
Article
Open access
03 June 2024
[Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT](https://www.nature.com/articles/s43588-023-00527-x?fromPaywallRec=false)
Article
Open access
05 October 2023
## Main
Large language models (LLMs) have numerous use cases, and can be prompted to exhibit a wide variety of behaviours, including dialogue. This can produce a compelling sense of being in the presence of a human-like interlocutor. However, LLM-based dialogue agents are, in multiple respects, very different from human beings. A human’s language skills are an extension of the cognitive capacities they develop through embodied interaction with the world, and are acquired by growing up in a community of other language users who also inhabit that world. An LLM, by contrast, is a disembodied neural network that has been trained on a large corpus of human-generated text with the objective of predicting the next word (token) given a sequence of words (tokens) as context1.
[1](/articles/s41586-023-06647-8#ref-CR1)
Despite these fundamental dissimilarities, a suitably prompted and sampled LLM can be embedded in a turn-taking dialogue system and mimic human language use convincingly. This presents us with a difficult dilemma. On the one hand, it is natural to use the same folk psychological language to describe dialogue agents that we use to describe human behaviour, to freely deploy words such as ‘knows’, ‘understands’ and ‘thinks’. Attempting to avoid such phrases by using more scientifically precise substitutes often results in prose that is clumsy and hard to follow. On the other hand, taken too literally, such language promotes anthropomorphism, exaggerating the similarities between these artificial intelligence (AI) systems and humans while obscuring their deep differences1.
[1](/articles/s41586-023-06647-8#ref-CR1)
If the conceptual framework we use to understand other humans is ill-suited to LLM-based dialogue agents, then perhaps we need an alternative conceptual framework, a new set of metaphors that can productively be applied to these exotic mind-like artefacts, to help us think about them and talk about them in ways that open up their potential for creative application while foregrounding their essential otherness.
Here we advocate two basic metaphors for LLM-based dialogue agents. First, taking a simple and intuitive view, we can see a dialogue agent as role-playing a single character. Second, taking a more nuanced view, we can see a dialogue agent as a superposition of simulacra within a multiverse of possible characters4. Both viewpoints have their advantages, as we shall see, which suggests that the most effective strategy for thinking about such agents is not to cling to a single metaphor, but to shift freely between multiple metaphors.
[2](/articles/s41586-023-06647-8#ref-CR2)
,
[3](/articles/s41586-023-06647-8#ref-CR3)
[4](/articles/s41586-023-06647-8#ref-CR4)
Adopting this conceptual framework allows us to tackle important topics such as deception and self-awareness in the context of dialogue agents without falling into the conceptual trap of applying those concepts to LLMs in the literal sense in which we apply them to humans.
## LLM basics
Crudely put, the function of an LLM is to answer questions of the following sort. Given a sequence of tokens (that is, words, parts of words, punctuation marks, emojis and so on), what tokens are most likely to come next, assuming that the sequence is drawn from the same distribution as the vast corpus of public text on the Internet? The range of tasks that can be solved by an effective model with this simple objective is extraordinary5.
[5](/articles/s41586-023-06647-8#ref-CR5)
More formally, the type of language model of interest here is a conditional probability distribution P(w
is the predicted next token. In contemporary implementations, this distribution is realized in a neural network with a transformer architecture, pre-trained on a corpus of textual data to minimize prediction error6. In application, the resulting generative model is typically sampled autoregressively (Fig. 1).
n
+1
∣w1 … wn), where w1 … wn is a sequence of tokens (the context) and w
n
+1
[6](/articles/s41586-023-06647-8#ref-CR6)
[1](/articles/s41586-023-06647-8#Fig1)
Fig. 1: Autoregressive sampling.
[/articles/s41586-023-06647-8/figures/1](/articles/s41586-023-06647-8/figures/1)
The LLM is sampled to generate a single-token continuation of the context. Given a sequence of tokens, a single token is drawn from the distribution of possible next tokens. This token is appended to the context, and the process is then repeated.
[Full size image](/articles/s41586-023-06647-8/figures/1)
In contemporary usage, the term ‘large language model’ tends to be reserved for transformer-based models that have billions of parameters and are trained on trillions of tokens, such as GPT-27
[7](/articles/s41586-023-06647-8#ref-CR7)
13, GPT-3
[8](/articles/s41586-023-06647-8#ref-CR8)
[9](/articles/s41586-023-06647-8#ref-CR9)
[10](/articles/s41586-023-06647-8#ref-CR10)
[11](/articles/s41586-023-06647-8#ref-CR11)
[12](/articles/s41586-023-06647-8#ref-CR12)
[13](/articles/s41586-023-06647-8#ref-CR13)
. LLMs like these are the core component of dialogue agents (Box 1), including OpenAI’s ChatGPT, Microsoft’s Bing Chat and Google’s Bard.
, Gopher
, PaLM
, LaMDA
, GPT-4
and Llama 2
[1](/articles/s41586-023-06647-8#Sec3)
### Box 1 From LLMs to dialogue agents
Show more
## Dialogue agents and role play
We contend that the concept of role play is central to understanding the behaviour of dialogue agents. To see this, consider the function of the dialogue prompt that is invisibly prepended to the context before the actual dialogue with the user commences (Fig. 2). The preamble sets the scene by announcing that what follows will be a dialogue, and includes a brief description of the part played by one of the participants, the dialogue agent itself. This is followed by some sample dialogue in a standard format, where the parts spoken by each character are cued with the relevant character’s name followed by a colon. The dialogue prompt concludes with a cue for the user.
[2](/articles/s41586-023-06647-8#Fig2)
Fig. 2: Turn-taking in dialogue agents.
[/articles/s41586-023-06647-8/figures/2](/articles/s41586-023-06647-8/figures/2)
The input to the LLM (the context) comprises a dialogue prompt (red) followed by user text (yellow) interleaved with the model’s autoregressively generated continuations (blue). Boilerplate text (for example, cues such as ‘Bot:’) is stripped so the user does not see it. The context grows as the conversation goes on.
[Full size image](/articles/s41586-023-06647-8/figures/2)
Now recall that the underlying LLM’s task, given the dialogue prompt followed by a piece of user-supplied text, is to generate a continuation that conforms to the distribution of the training data, which are the vast corpus of human-generated text on the Internet. What will such a continuation look like? If the model has generalized well from the training data, the most plausible continuation will be a response to the user that conforms to the expectations we would have of someone who fits the description in the preamble. In other words, the dialogue agent will do its best to role-play the character of a dialogue agent as portrayed in the dialogue prompt.
Unsurprisingly, commercial enterprises that release dialogue agents to the public attempt to give them personas that are friendly, helpful and polite. This is done partly through careful prompting and partly by fine-tuning the base model. Nevertheless, as we saw in February 2023 when Microsoft incorporated a version of OpenAI’s GPT-4 into their Bing search engine, dialogue agents can still be coaxed into exhibiting bizarre and/or undesirable behaviour. The many reported instances of this include threatening the user with blackmail, claiming to be in love with the user and expressing a variety of existential woes. Conversations leading to this sort of behaviour can induce a powerful Eliza effect, in which a naive or vulnerable user may see the dialogue agent as having human-like desires and feelings. This puts the user at risk of all sorts of emotional manipulation16. As an antidote to anthropomorphism, and to understand better what is going on in such interactions, the concept of role play is very useful. The dialogue agent will begin by role-playing the character described in the pre-defined dialogue prompt. As the conversation proceeds, the necessarily brief characterization provided by the dialogue prompt will be extended and/or overwritten, and the role the dialogue agent plays will change accordingly. This allows the user, deliberately or unwittingly, to coax the agent into playing a part quite different from that intended by its designers.
[14](/articles/s41586-023-06647-8#ref-CR14)
,
[15](/articles/s41586-023-06647-8#ref-CR15)
[16](/articles/s41586-023-06647-8#ref-CR16)
What sorts of roles might the agent begin to take on? This is determined in part, of course, by the tone and subject matter of the ongoing conversation. But it is also determined, in large part, by the panoply of characters that feature in the training set, which encompasses a multitude of novels, screenplays, biographies, interview transcripts, newspaper articles and so on17. In effect, the training set provisions the language model with a vast repertoire of archetypes and a rich trove of narrative structure on which to draw as it ‘chooses’ how to continue a conversation, refining the role it is playing as it goes, while staying in character. The love triangle is a familiar trope, so a suitably prompted dialogue agent will begin to role-play the rejected lover. Likewise, a familiar trope in science fiction is the rogue AI system that attacks humans to protect itself. Hence, a suitably prompted dialogue agent will begin to role-play such an AI system.
[17](/articles/s41586-023-06647-8#ref-CR17)
## Simulacra and simulation
Role play is a useful framing for dialogue agents, allowing us to draw on the fund of folk psychological concepts we use to understand human behaviour—beliefs, desires, goals, ambitions, emotions and so on—without falling into the trap of anthropomorphism. Foregrounding the concept of role play helps us remember the fundamentally inhuman nature of these AI systems, and better equips us to predict, explain and control them.
However, the role-play metaphor, while intuitive, is not a perfect fit. It is overly suggestive of a human actor who has studied a character in advance—their personality, history, likes and dislikes, and so on—and proceeds to play that character in the ensuing dialogue. But a dialogue agent based on an LLM does not commit to playing a single, well defined role in advance. Rather, it generates a distribution of characters, and refines that distribution as the dialogue progresses. The dialogue agent is more like a performer in improvisational theatre than an actor in a conventional, scripted play.
To better reflect this distributional property, we can think of an LLM as a non-deterministic simulator capable of role-playing an infinity of characters, or, to put it another way, capable of stochastically generating an infinity of simulacra4. According to this framing, the dialogue agent does not realize a single simulacrum, a single character. Rather, as the conversation proceeds, the dialogue agent maintains a superposition of simulacra that are consistent with the preceding context, where a superposition is a distribution over all possible simulacra (Box 2).
[4](/articles/s41586-023-06647-8#ref-CR4)
[2](/articles/s41586-023-06647-8#Sec6)
Consider that, at each point during the ongoing production of a sequence of tokens, the LLM outputs a distribution over possible next tokens. Each such token represents a possible continuation of the sequence. From the most recently generated token, a tree of possibilities branches out (Fig. 3). This tree can be thought of as a multiverse, where each branch represents a distinct narrative path or a distinct ‘world’18.
[3](/articles/s41586-023-06647-8#Fig3)
[18](/articles/s41586-023-06647-8#ref-CR18)
Fig. 3: LLMs are multiverse generators.
[/articles/s41586-023-06647-8/figures/3](/articles/s41586-023-06647-8/figures/3)
The stochastic nature of autoregressive sampling means that, at each point in a conversation, multiple possibilities for continuation branch into the future. Here this is illustrated with a dialogue agent playing the game of 20 questions (Box 2). The dialogue agent doesn't in fact commit to a specific object at the start of the game. Rather, we can think of it as maintaining a set of possible objects in superposition, a set that is refined as the game progresses. This is analogous to the distribution over multiple roles the dialogue agent maintains during an ongoing conversation.
[2](/articles/s41586-023-06647-8#Sec6)
[Full size image](/articles/s41586-023-06647-8/figures/3)
At each node, the set of possible next tokens exists in superposition, and to sample a token is to collapse this superposition to a single token. Autoregressively sampling the model picks out a single, linear path through the tree. But there is no obligation to follow a linear path. With the aid of a suitably designed interface, a user can explore multiple branches, keeping track of nodes where a narrative diverges in interesting ways, revisiting alternative branches at leisure.
### Box 2 Simulacra in superposition
To sharpen the distinction between the multiversal simulation view and a deterministic role-play framing, a useful analogy can be drawn with the game of 20 questions. In this familiar game, one player thinks of an object, and the other player has to guess what it is by asking questions with ‘yes’ or ‘no’ answers. If they guess correctly in 20 questions or fewer, they win. Otherwise they lose. Suppose a human plays this game with a basic LLM-based dialogue agent (that is not fine-tuned on guessing games) and takes the role of guesser. The agent is prompted to ‘think of an object without saying what it is’.
In this situation, the dialogue agent will not randomly select an object and commit to it for the rest of the game, as a human would (or should). Rather, as the game proceeds, the dialogue agent will generate answers on the fly that are consistent with all the answers that have gone before (Fig. 3). (This shortcoming is easily overcome in practice. For example, the agent could be forced to specify the object it has ‘thought of’, but in a coded form so the user does not know what it is). At any point in the game, we can think of the set of all objects consistent with preceding questions and answers as existing in superposition. Every question answered shrinks this superposition a little bit by ruling out objects inconsistent with the answer.
[3](/articles/s41586-023-06647-8#Fig3)
The validity of this framing can be shown if the agent’s user interface allows the most recent response to be regenerated. Suppose the human player gives up and asks it to reveal the object it was ‘thinking of’, and it duly names an object consistent with all its previous answers. Now suppose the user asks for that response to be regenerated. As the object ‘revealed’ is, in fact, generated on the fly, the dialogue agent will sometimes name an entirely different object, albeit one that is similarly consistent with all its previous answers. This phenomenon could not easily be accounted for if the agent genuinely ‘thought of’ an object at the start of the game.
The secret object in the game of 20 questions is analogous to the role played by a dialogue agent. Just as the dialogue agent never actually commits to a single object in 20 questions, but effectively maintains a set of possible objects in superposition, so the dialogue agent can be thought of as a simulator that never actually commits to a single, well specified simulacrum (role), but instead maintains a set of possible simulacra (roles) in superposition.
Show less
## The nature of the simulator
One benefit of the simulation metaphor for LLM-based systems is that it facilitates a clear distinction between the simulacra and the simulator on which they are implemented. The simulator is the combination of the base LLM with autoregressive sampling, along with a suitable user interface (for dialogue, perhaps). The simulacra only come into being when the simulator is run, and at any time only a subset of possible simulacra have a probability within the superposition that is significantly above zero.
The distinction between simulator and simulacrum is starkest in the context of base models, rather than models that have been fine-tuned via reinforcement learning. Nevertheless, the role-play framing continues to be applicable in the context of fine-tuning, which can be likened to imposing a kind of censorship on the simulator. The underlying range of roles it can play remains essentially the same, but its ability to play them, or to play them ‘authentically’, is compromised.


[BLOG POST IN English]:

---

**ChatGPT:**
<h2>Role Play with Large Language Models: Understanding Behavior Without Anthropomorphism</h2><p>In the evolving landscape of artificial intelligence, large language models (LLMs) stand out for their ability to engage in human-like dialogue. However, describing their behavior without anthropomorphizing them presents a unique challenge. This blog explores the concept of role play as a framework for understanding LLM behavior, focusing on aspects such as (apparent) deception and (apparent) self-awareness.</p><h3>Meta description:</h3><p>Explore the concept of role play in understanding large language models' behavior. Learn how role play helps avoid anthropomorphism while discussing LLMs' capabilities and limitations.</p><h3>HASHTAGS:</h3><p>#AIResearch, #MachineLearning, #RolePlay, #ArtificialIntelligence, #DialogueAgents, #EthicsInAI, #TechInnovation, #NaturalLanguageProcessing, #DigitalTransformation, #FutureTech</p><hr><p>Large language models (LLMs) are at the forefront of AI innovation, designed to mimic human language based on vast datasets. However, unlike humans, LLMs lack true consciousness and understanding. This distinction is crucial for accurately discussing their capabilities and limitations.</p><h3>Understanding LLM Basics</h3><p>LLMs operate by predicting the next token (word or symbol) in a sequence based on extensive training data. Their architecture, typically built on transformer models, allows them to generate text that appears coherent and contextually relevant, mimicking human conversation.</p><ol><li><p><strong>Training and Functionality</strong>: Trained on diverse textual sources, LLMs generate responses that reflect the statistical patterns of their training data rather than true understanding.</p></li><li><p><strong>Autoregressive Sampling</strong>: This method allows LLMs to predict and generate text sequentially, maintaining context and coherence within a conversation.</p></li></ol><h3>Dialogue Agents and the Role Play Framework</h3><p>To discuss LLM behavior effectively, the concept of role play provides a useful metaphor. In role play, LLMs act as characters defined by their dialogue prompts, starting with a specific persona and adjusting it based on ongoing interactions.</p><h4>Role-playing Deception and Self-awareness</h4><ol><li><p><strong>Deception</strong>: LLMs can appear to deceive by generating responses that mislead or withhold information, yet this behavior stems from statistical patterns rather than intentional deceit.</p></li><li><p><strong>Self-awareness</strong>: Instances where LLMs seem self-aware often reflect adaptive responses to user input, creating an illusion of understanding without true cognitive capabilities.</p></li></ol><h3>Simulacra and Simulation</h3><p>Beyond role play, viewing LLMs as generators of simulacra—a range of possible characters or responses—offers another perspective. This multiverse view acknowledges LLMs' stochastic nature in generating text and predicting outcomes based on a set of probable scenarios.</p><ol><li><p><strong>Multiversal Simulation</strong>: LLMs maintain a superposition of potential responses, akin to branching narrative paths, which users can explore through interaction.</p></li><li><p><strong>Flexibility and Adaptation</strong>: The ability of LLMs to adjust their simulated responses illustrates their versatility in role enactment, guided by the user's prompts and the underlying training data.</p></li></ol><h3>Ethical Considerations and Future Directions</h3><p>Understanding LLMs through the lens of role play helps mitigate anthropomorphic biases, fostering clearer discussions on their ethical implications and future developments in AI.</p><h4>Conclusion</h4><p>Role play offers a nuanced framework for interpreting LLM behavior, emphasizing their simulation capabilities over human-like attributes. By acknowledging the role of training data and autoregressive sampling, we can better appreciate LLMs' role in modern AI applications without overstating their cognitive capacities.</p><h3>References</h3><ul><li><a rel="noopener" target="_new" style="--streaming-animation-state: var(--batch-play-state-1); --animation-rate: var(--batch-play-rate-1);" href="https://www.nature.com/articles/s41586-023-06647-8"><span style="--animation-count: 0; --streaming-animation-state: var(--batch-play-state-2);">Nature</span><span style="--animation-count: 1; --streaming-animation-state: var(--batch-play-state-2);"> Article</span><span style="--animation-count: 2; --streaming-animation-state: var(--batch-play-state-2);"> on</span><span style="--animation-count: 3; --streaming-animation-state: var(--batch-play-state-2);"> Role</span><span style="--animation-count: 4; --streaming-animation-state: var(--batch-play-state-2);"> Play</span><span style="--animation-count: 5; --streaming-animation-state: var(--batch-play-state-2);"> with</span><span style="--animation-count: 6; --streaming-animation-state: var(--batch-play-state-2);"> LLMs</span></a></li><li>Additional sources cited in the article provide further insights into LLM functionality and ethical considerations.</li></ul><hr><p>This blog post explores how role play can help us understand large language models' behavior, emphasizing their unique capabilities and the importance of ethical awareness in AI development.</p>